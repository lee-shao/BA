{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c641584-6ca0-4c44-babf-21c902cc5e01",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 305\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# Wenn k gleich dem höchsten existierenden k-Wert ist, lösche die entsprechenden Einträge in der CSV\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m max_k:\n\u001b[1;32m--> 305\u001b[0m     df_existing \u001b[38;5;241m=\u001b[39m df_existing[df_existing[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m k]\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;66;03m#df = pd.read_csv(os.path.join(data_folder, filename), sep=';', index_col=False)\u001b[39;00m\n\u001b[0;32m    310\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_folder, filename), sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:418\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'k'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import winsound\n",
    "\n",
    "data_folder = './data'\n",
    "\n",
    "# Gesamtzeitmessung starten\n",
    "start_time_total = time.time()\n",
    "\n",
    "# Datensätze einlesen und sortieren\n",
    "def read_and_sort_datasets(folder):\n",
    "    datasets = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            parts = filename.replace('.csv', '').split('_')\n",
    "            k = int(parts[1][1:])\n",
    "            l = int(parts[2][1:])\n",
    "            datasets.append((k, l, filename))\n",
    "    datasets.sort() \n",
    "    return datasets\n",
    "\n",
    "sorted_datasets = read_and_sort_datasets(data_folder)\n",
    "# print(sorted_datasets)\n",
    "\n",
    "# Preprocessing-Funktion für Intervalle und Mengen\n",
    "def preprocessing(df):\n",
    "    def process_value(value):\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        if isinstance(value, str):\n",
    "            value = value.strip()\n",
    "            if value.startswith('[') and value.endswith('['):\n",
    "                try:\n",
    "                    start, end = map(float, value[1:-1].split(','))\n",
    "                    return (start + end) / 2\n",
    "                except ValueError:\n",
    "                    return np.nan\n",
    "            elif value.startswith('{') and value.endswith('}'):\n",
    "                try:\n",
    "                    values = list(map(float, value[1:-1].split(',')))\n",
    "                    return np.mean(values)\n",
    "                except ValueError:\n",
    "                    return np.nan\n",
    "            else:\n",
    "                try:\n",
    "                    return float(value)\n",
    "                except ValueError:\n",
    "                    return np.nan\n",
    "        return value\n",
    "    \n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(process_value)\n",
    "    return df\n",
    "\n",
    "# Funktion zum Extrahieren des ersten numerischen Werts\n",
    "def extract_numeric(value):\n",
    "    value = str(value).strip()\n",
    "    if value == \"*\":\n",
    "        return np.nan\n",
    "    elif value.startswith('[') and value.endswith('['):\n",
    "        interval = value.strip('[').strip('[').split(', ')\n",
    "        return int(interval[0])\n",
    "    elif value.startswith('{') and value.endswith('}'):\n",
    "        set_values = [int(v.strip()) for v in value.strip('{}').split(',')]\n",
    "        return min(set_values)\n",
    "    else:\n",
    "        try:\n",
    "            return int(value)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "# Anzahl plotten\n",
    "def plot_cardio_count(df, output_dir):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # sns.set(rc={'axes.facecolor':'c0c0c0', 'figure.facecolor':'lightblue'})\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x=df[\"cardio\"].value_counts().index, y=df[\"cardio\"].value_counts())\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pie(x=df[\"cardio\"].value_counts(), autopct=\"%.1f%%\", pctdistance=0.8,\n",
    "            labels= df[\"cardio\"].value_counts().index, shadow=True, explode=[0.05,0.05])\n",
    "    plt.suptitle(\"Cardiovascular Disease\", fontsize=16)\n",
    "    plt.savefig(os.path.join(output_dir, 'cardio_count.png'))\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Features plotten\n",
    "def plot_column(df, column_name, output_dir):  \n",
    "    plt.figure(figsize=(20, 7))\n",
    "\n",
    "    # Extrahiere den ersten numerischen Wert und sortiere\n",
    "    df[f'{column_name}_num'] = df[column_name].apply(extract_numeric)\n",
    "    df_sorted = df.dropna(subset=[f'{column_name}_num']).sort_values(by=f'{column_name}_num')\n",
    "    \n",
    "    chart_num = df_sorted[column_name].value_counts().reindex(df_sorted[column_name].unique())\n",
    "    chart = sns.barplot(x = chart_num.index, y = chart_num)\n",
    "    \n",
    "    for p in chart.patches:\n",
    "        chart.annotate(format(p.get_height(), '.0f'), \n",
    "                       (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                       ha = 'center', va = 'center', \n",
    "                       xytext = (0, 15), \n",
    "                       textcoords = 'offset points',\n",
    "                       fontsize=8,\n",
    "                       rotation=90\n",
    "                      )\n",
    "\n",
    "    chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "    plt.title(f'Distribution of {column_name}')\n",
    "    plt.savefig(os.path.join(output_dir, f'{column_name}.png'))\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Ergebnisse plotten\n",
    "\n",
    "# Confusion Matrix\n",
    "def plot_confusion_matrix(y_test, y_pred, classifier_name, output_dir):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"0\", \"1\"])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {classifier_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig(os.path.join(output_dir, f'confusion_matrix_{classifier_name}.png'))\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "# ROC AUC Kurve\n",
    "def plot_roc_auc(y_test, y_proba, roc_auc, classifier_name, output_dir):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC AUC - {classifier_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(os.path.join(output_dir, f'roc_auc_{classifier_name}.png'))\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "# Feature Importances (nur für Random Forest und Gradient Boosting)\n",
    "def plot_feature_importances(classifier, classifier_name, X_train, X, output_dir):\n",
    "    if hasattr(classifier, 'feature_importances_'):\n",
    "        importances = classifier.feature_importances_\n",
    "        #indices = np.argsort(importances)[::-1]\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.title(f'Feature Importances - {classifier_name}')\n",
    "        bars = plt.bar(range(X_train.shape[1]), importances, align='center')\n",
    "        plt.xticks(range(X_train.shape[1]), X.columns, rotation=90)\n",
    "        plt.xlim([-1, X_train.shape[1]])\n",
    "\n",
    "        for bar, importance in zip(bars, importances):\n",
    "            yval = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(importance, 2), ha='center', va='bottom')\n",
    "        \n",
    "        plt.savefig(os.path.join(output_dir, f'feature_importances_{classifier_name}.png'))\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "\n",
    "# Korrelations heatmap erstellen\n",
    "def plot_correlation_heatmap(X, classifier_name, output_dir):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    correlation_matrix = X.corr()\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    im = ax.imshow(correlation_matrix, cmap='Blues')\n",
    "    ax.set_xticks(np.arange(len(X.columns)))\n",
    "    ax.set_yticks(np.arange(len(X.columns)))\n",
    "    ax.set_xticklabels(X.columns, rotation=45)\n",
    "    ax.set_yticklabels(X.columns)\n",
    "    # Korrelationswerte ausgeben\n",
    "    for i in range(len(X.columns)):\n",
    "        for j in range(len(X.columns)):\n",
    "            text = ax.text(j, i, f\"{correlation_matrix.iloc[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "    plt.colorbar(im)\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.savefig(os.path.join(output_dir, f'correlation_heatmap_{classifier_name}.png'))\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def classify_and_evaluate(df, classifier_name, clf, output_dir):\n",
    "    target_column = 'cardio'\n",
    "    if target_column not in df.columns:\n",
    "        print(f\"Die Zielspalte '{target_column}' wurde nicht im DataFrame gefunden.\")\n",
    "        return\n",
    "\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Feature- und Zielvariablen definieren\n",
    "    X = df.drop(columns=['cardio'])\n",
    "    y = df['cardio']\n",
    "\n",
    "    # Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Daten normalisieren\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Zeitmesser starten\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Modelltraining\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Zeitmesser stoppen\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # Vorhersagen\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_proba = clf.predict_proba(X_test)[:, 1] if hasattr(clf, \"predict_proba\") else None\n",
    "    \n",
    "    # Classification Report\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "    }\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Speichern der Ergebnisse\n",
    "    with open(os.path.join(output_dir, f'classification_report_{classifier_name}.txt'), 'w') as f:\n",
    "        f.write(report)\n",
    "        f.write(f'\\nAccuracy: {metrics[\"accuracy\"]}\\n')\n",
    "        f.write(f'ROC AUC: {metrics[\"roc_auc\"]}\\n')\n",
    "        f.write(f\"Laufzeit: {total_time:.2f} Sekunden\\n\")\n",
    "\n",
    "    # Plotten der Ergebnisse\n",
    "    plot_confusion_matrix(y_test, y_pred, classifier_name, output_dir)\n",
    "    plot_roc_auc(y_test, y_proba, metrics['roc_auc'], classifier_name, output_dir)\n",
    "    plot_feature_importances(clf, classifier_name, X_train, X, output_dir)\n",
    "    plot_correlation_heatmap(X, classifier_name, output_dir)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "# EDA & Classifying\n",
    "results = []\n",
    "num_iterations = 100\n",
    "\n",
    "output_base_dir = 'results'\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "classifiers = ['lr', 'rf', 'gb']\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "\n",
    "# Überprüfen, welche k- und l-Werte bereits vorhanden sind\n",
    "existing_files = [f for f in os.listdir(output_base_dir) if f.startswith('cvd_k')]\n",
    "existing_k_l_pairs = []\n",
    "\n",
    "for f in existing_files:\n",
    "    parts = f.split('_')\n",
    "    \n",
    "    # Überprüfen, ob der Name aus 3 Teilen besteht und das richtige Format hat\n",
    "    if len(parts) == 3 and parts[0] == 'cvd' and parts[1].startswith('k') and parts[2].startswith('l'):\n",
    "        try:\n",
    "            # Extrahiere die numerischen Werte von k und l\n",
    "            k = int(parts[1][1:])  # Entfernt das 'k' und konvertiert den Rest in eine Ganzzahl\n",
    "            l = int(parts[2][1:])  # Entfernt das 'l' und konvertiert den Rest in eine Ganzzahl\n",
    "            existing_k_l_pairs.append((k, l))\n",
    "        except ValueError:\n",
    "            continue  # Falls es einen Fehler bei der Konvertierung gibt, überspringe diese Datei\n",
    "\n",
    "# Bestimmen des höchsten k-Werts\n",
    "if existing_k_l_pairs:\n",
    "    max_k, max_l = max(existing_k_l_pairs)\n",
    "else:\n",
    "    max_k, max_l = -1, -1\n",
    "\n",
    "\n",
    "# Lesen der bestehenden CSV-Datei\n",
    "csv_file_path = f'{output_base_dir}/classification_results_multiple_iterations.csv'\n",
    "if os.path.exists(csv_file_path):\n",
    "    df_existing = pd.read_csv(csv_file_path)\n",
    "else:\n",
    "    df_existing = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Schleife über die Datensätze\n",
    "for k, l, filename in sorted_datasets:\n",
    "    # Wenn k kleiner ist als der höchste existierende k-Wert, überspringe diesen Durchlauf\n",
    "    if k < max_k:\n",
    "        continue\n",
    "\n",
    "    # Wenn k gleich dem höchsten existierenden k-Wert ist, lösche die entsprechenden Einträge in der CSV\n",
    "    if k == max_k:\n",
    "        df_existing = df_existing[df_existing['k'] != k]\n",
    "    \n",
    "        \n",
    "    try:\n",
    "        #df = pd.read_csv(os.path.join(data_folder, filename), sep=';', index_col=False)\n",
    "        df = pd.read_csv(os.path.join(data_folder, filename), sep=',', index_col=False)\n",
    "        print(f'Successfully read {filename}')\n",
    "\n",
    "        output_dir = f'results/cvd_k{k}_l{l}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "        # id entfernen\n",
    "        df = df.drop('id', axis=1)\n",
    "\n",
    "        # Zeilen mit \"*\" entfernen\n",
    "        df = df[~df.isin(['*']).any(axis=1)]\n",
    "\n",
    "        # EDA plotten\n",
    "        plot_cardio_count(df, output_dir)\n",
    "        for column in df.columns:\n",
    "            plot_column(df, column, output_dir)\n",
    "                \n",
    "        # Alle Spalten, die mit '_num' enden, entfernen\n",
    "        df = df[[col for col in df.columns if not col.endswith('_num')]]\n",
    "\n",
    "        # Zeilen mit NaN Werten entfernen\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Klassifizierung mit verschiedenen Modellen\n",
    "        classifiers = {\n",
    "            'lr': LogisticRegression(max_iter=10000),\n",
    "            'rf': RandomForestClassifier(),\n",
    "            'gb': GradientBoostingClassifier()\n",
    "        }\n",
    "\n",
    "        df = preprocessing(df)\n",
    "\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            for i in range(num_iterations):\n",
    "                print(f'Classifying with {clf_name} for {filename}, iteration {i+1}/{num_iterations}')\n",
    "                metrics = classify_and_evaluate(df, clf_name, clf, output_dir)\n",
    "\n",
    "                results.append({\n",
    "                    'k':k, \n",
    "                    'l':l,\n",
    "                    'filename': filename,\n",
    "                    'classifier': clf_name,\n",
    "                    **metrics\n",
    "                })\n",
    "\n",
    "                # Fortschritt nach jeder Iteration anhängen\n",
    "                df_results = pd.DataFrame(results)\n",
    "                \n",
    "                # Löschen der bestehenden Einträge für den aktuellen k-Wert und Anhängen neuer Ergebnisse\n",
    "                df_existing = pd.concat([df_existing, df_results], ignore_index=True)\n",
    "                df_existing.to_csv(csv_file_path, index=False)\n",
    "                \n",
    "                results = []  # Clear results after writing to CSV to avoid duplications\n",
    "    \n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f'Error parsing {filename}: {e}')\n",
    "        continue\n",
    "\n",
    "winsound.Beep(1000, 1000)\n",
    "\n",
    "\n",
    "# Endzeit messen\n",
    "end_time_total = time.time()\n",
    "\n",
    "# Berechne die verstrichene Zeit\n",
    "elapsed_time_total = end_time_total - start_time_total\n",
    "\n",
    "hours_total, rem_total = divmod(elapsed_time_total, 3600)\n",
    "minutes_total, seconds_total = divmod(rem_total, 60)\n",
    "\n",
    "print(f\"Gesamte Ausführung hat {int(hours_total)} Stunden, {int(minutes_total)} Minuten, und {seconds_total:.2f} Sekunden gedauert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86e782",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f707c-f38b-4ea3-9525-f8671fb4e2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
